# mini-transformer-lab

A compact transformer-based language model built from scratch in pure PyTorch â€” designed for learning, tinkering, and really understanding how LLMs work under the hood.

> ðŸ”¬ **Built with curiosity, debugged with persistence**
> 
> Almost every error and bug in this project was debugged (sometimes painfully) with a bit of help from AI tools â€” and yeah, my brain had its fair share of contributions too in debugging, coding, and the whole idea. Because honestly, what else would you expect from a self-taught Python programmer building their own LLM?



## ðŸš€ Quick Start

### Installation

 ```bash
# Clone this repository
git clone https://github.com/Aranya-Marjara/mini-transformer-lab.git
cd mini-transformer-lab

# Install dependencies (installing in a virtual environment is highly recommended)
pip install torch 
 ```
### Create a file 
 ```
nano your_text.txt
 ```

###  Data Requirements

Your training file should be large enough for the context length:
- **Minimum size**: `(batch_size Ã— context_length + 1)` characters
- **Example**: For batch_size=32, context_length=256 â†’ need ~8,200+ characters
- **Small data?** Use: `--context_length 64 --batch_size 8`


## Paste the text you want to train your model on. (Your data should be large enough for the context length!)
## Hereâ€™s an example â€” use a smaller context length.
 ```
Artificial intelligence has changed the world of technology forever. From simple rule-based systems to advanced large language models, AI continues to evolve with astonishing speed. The transformer architecture revolutionized the way machines understand language, allowing them to capture long-range dependencies and contextual meaning with ease.

Understanding how these models work under the hood is a fascinating challenge. Each token, attention head, and layer contributes to the model's ability to reason, summarize, and generate text. Building a small transformer model from scratch is not just an engineering task but also an educational journey into the core mechanics of intelligence.

Training even a mini transformer on your own data helps reveal how neural networks learn from patterns. As the loss goes down, the model starts to grasp structure â€” words begin to align, and meaning starts to emerge. These tiny experiments reflect, in miniature, what the biggest LLMs in the world are doing at scale.

Open-source research and tinkering allow anyone to learn how these systems work. Sharing your code publicly means that others can build on your work, improve it, and contribute ideas. True progress happens when knowledge is free, transparent, and collaborative.

This text exists to provide enough data for your mini-transformer-lab model to train successfully, test attention layers, and generate a few coherent lines. Keep experimenting, because every small step adds up to something bigger in the world of AI.
 ```


### Basic Usage
# Train a new model on your text (use smaller context length â€” recommended)
 ```
python3 mini-transformer-lab.py train --data your_text.txt --epochs 10 --context_length 64 --batch_size 8
 ```

# Generate some text
 ```
python3 mini-transformer-lab.py generate --checkpoint checkpoint_epoch_10.pt --prompt "The future of AI is"
 ```

### Quick Test
 ```bash
# Create a tiny test file
echo "Hello world! This is a test." > test.txt

# Train quickly
python3 mini-transformer-lab.py train --data test.txt --epochs 5 --context_length 16 --batch_size 2

# Generate
python3 mini-transformer-lab.py generate --checkpoint checkpoint_epoch_5.pt --prompt "Hello"
 ```


### Troubleshooting

- **"Data too short for context length"**  
  Use smaller context: `--context_length 64 --batch_size 8`  
  Or get more data or use the sample text above.

- **"Checkpoint not found"**  
  Use actual checkpoint names: `checkpoint_epoch_5.pt`, `checkpoint_epoch_10.pt`

- **"First outputs are gibberish"**  
  This is normal for early training!  
  Train for more epochs (20â€“50) or use a larger dataset for more coherent text.


## Transformer Architecture

 ```Input: "Hello world"
     â†“
Tokenize: [23, 45, 12, 67]
     â†“
Embeddings + Positional Encoding
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Multi-Head Self-Attention      â”‚ â†â”€â”€â”
â”‚  (Causal Masking)               â”‚    â”‚ Repeat 4x
â”‚  LayerNorm + Feed Forward       â”‚ â†â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
Output Logits â†’ Next Token Prediction
 ```

## Sampling Strategies

 ```Temperature Sampling:
   logits = logits / temperature
   â”Œâ”€ Hot (0.8) â†’ Creative
   â”œâ”€ Warm (1.0) â†’ Balanced  
   â””â”€ Cold (1.5) â†’ Conservative

Top-k Sampling:
   â”Œâ”€ Keep only top k tokens
   â””â”€ k=50 â†’ Diverse but coherent

Top-p (Nucleus) Sampling:
   â”Œâ”€ Keep tokens until cumulative
   â”‚  probability reaches p
   â””â”€ p=0.9 â†’ Dynamic vocabulary
 ```
## Training Flow
 ```
ðŸ“– Load Text Data
   â†“
ðŸ”¡ Tokenize Characters
   â†“
ðŸ”„ For each epoch:
   â”œâ”€â”€ ðŸŽ² Sample training batch
   â”œâ”€â”€ ðŸ§  Forward pass
   â”œâ”€â”€ ðŸ“‰ Compute loss
   â”œâ”€â”€ â†ªï¸ Backward pass
   â””â”€â”€ âš™ï¸ Update weights
   â†“
ðŸ’¾ Save checkpoint
 ```
## Advanced Usage (Training with Custom Parameters)
 ```
python mini-transformer-lab.py train \
  --data shakespeare.txt \
  --epochs 20 \
  --batch_size 32 \
  --context_length 256 \
  --lr 0.0003 \
  --device cuda  # Use GPU if available
 ```
## Creative Generation
 ```
# Temperature controls creativity
python mini-transformer-lab.py generate \
  --checkpoint best_model.pt \
  --prompt "In a galaxy far away" \
  --temperature 0.7 \
  --top_k 40 \
  --top_p 0.9 \
  --max_new_tokens 200
 ```
## Resume Training
 ```
python mini-transformer-lab.py train \
  --data my_novel.txt \
  --resume checkpoint_epoch_15.pt \
  --epochs 25
 ```
## Model Architecture
 ```
MiniTransformer Config:
â”œâ”€â”€ ðŸ“ Context Length: 256 tokens
â”œâ”€â”€ ðŸŽ¯ Model Dimension: 256
â”œâ”€â”€ ðŸ‘¥ Attention Heads: 8
â”œâ”€â”€ ðŸ—ï¸ Layers: 4
â”œâ”€â”€ ðŸ§  Feed Forward: 1024
â””â”€â”€ ðŸ’§ Dropout: 0.1
 ```
## Design Philosophy
 ```
ðŸ§  Understanding > Scale
   â””â”€â”€ Built to learn, not to compete with GPT Models

ðŸ”§ Simplicity > Complexity  
   â””â”€â”€ Pure PyTorch, no fancy trainers

ðŸŽ¯ Education > Production
   â””â”€â”€ Readable code with clear explanations

ðŸ› Debugging > First-try perfection
   â””â”€â”€ Every error was a learning opportunity
 ```
