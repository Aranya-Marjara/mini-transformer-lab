# mini-transformer-lab

A compact transformer-based language model built from scratch in pure PyTorch â€” designed for learning, tinkering, and really understanding how LLMs work under the hood.

> ğŸ”¬ **Built with curiosity, debugged with persistence**
> 
> Almost every error and bug in this project was debugged (sometimes painfully) with a bit of help from AI tools â€” and yeah, my brain had its fair share of contributions too in debugging, coding, and the whole idea. Because honestly, what else would you expect from a self-taught Python programmer building their own LLM?

## ğŸ¯ What's Inside
mini-transformer-lab/
â”œâ”€â”€ ğŸ§  mini_llm.py # Main model & training script
â”œâ”€â”€ ğŸ“š example_data/ # Sample texts to train on
â”œâ”€â”€ ğŸ›ï¸ checkpoints/ # Saved model weights
â””â”€â”€ ğŸ”¬ experiments/ # Training logs & results

## ğŸš€ Quick Start

### Installation

```bash
# Clone this repository
git clone https://github.com/Aranya-Marjara/mini-transformer-lab.git
cd mini-transformer-lab

# Install dependencies (just PyTorch!)
pip install torch
